{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f4a32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  HATE SPEECH ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d152cd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Import Required Libraries\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9004f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load Dataset\n",
    "# ==============================\n",
    "\n",
    "# Load dataset from data folder\n",
    "df = pd.read_csv(\"hate_speech_dataset.csv\")\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77060306",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Basic Data Exploration\n",
    "# ==============================\n",
    "\n",
    "# Check dataset structure\n",
    "df.info()\n",
    "\n",
    "# Check shape\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Display column names\n",
    "print(\"\\nColumns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d39639",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Data Cleaning\n",
    "# ==============================\n",
    "\n",
    "# Columns containing boolean values\n",
    "bool_columns = [\n",
    "    'IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative',\n",
    "    'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist',\n",
    "    'IsSexist', 'IsHomophobic', 'IsReligiousHate', 'IsRadicalism'\n",
    "]\n",
    "\n",
    "# Convert TRUE/FALSE to 1/0\n",
    "df[bool_columns] = df[bool_columns].replace({True: 1, False: 0})\n",
    "\n",
    "print(\"Boolean columns converted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16de0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed846ce0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Count of hate categories\n",
    "\n",
    "hate_counts = df[bool_columns].sum()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "hate_counts.plot(kind='bar')\n",
    "plt.title(\"Count of Hate Categories\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90ab0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis: Toxic vs Abusive Relationship\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.violinplot(data=df, x=\"IsToxic\", y=\"IsAbusive\")\n",
    "plt.title(\"Relationship between Toxic and Abusive Comments\")\n",
    "plt.xlabel(\"IsToxic\")\n",
    "plt.ylabel(\"IsAbusive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd220a58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Racist vs Religious Hate Relationship\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(data=df, x=\"IsRacist\", y=\"IsReligiousHate\")\n",
    "plt.title(\"Racist vs Religious Hate Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe526d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Correaltion Heatmap\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df[bool_columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Between Hate Categories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476261c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##  Linear Regression: Predict Hate Severity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe92914",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_lr = df[features]\n",
    "y_lr = df['Total_Hate_Score']\n",
    "\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "    X_lr, y_lr, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_lr)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test_lr, y_pred_lr))\n",
    "print(\"R2 Score:\", r2_score(y_test_lr, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ea641",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d17d38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "features = [\n",
    "    'IsAbusive','IsThreat','IsProvocative','IsObscene',\n",
    "    'IsRacist','IsNationalist','IsSexist',\n",
    "    'IsHomophobic','IsReligiousHate','IsRadicalism'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['Hate_Label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c83b83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ROC Curve for Logistic Regression\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# Predict probabilities\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC values\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], linestyle='--')  # Random baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Hate Speech Classification\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f922027",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## MySQL Database Integration (Demonstration)\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"your_password\",\n",
    "    database=\"Automated_hate_speech\"\n",
    ")\n",
    "\n",
    "if db.is_connected():\n",
    "    print(\"Connected to MySQL successfully!\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b76f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Key Business Insights\n",
    "\n",
    "- Automated hate speech detection helps social media platforms proactively filter toxic comments, improving user safety and engagement.\n",
    "- High co-occurrence of abusive, racist, and religious hate content indicates the need for multi-category moderation strategies.\n",
    "- The Total Hate Score enables prioritization of highly offensive comments for faster review and action.\n",
    "- Binary classification of comments into hate vs non-hate supports real-time automated content moderation systems.\n",
    "- Data-driven hate speech analysis assists organizations in strengthening community guidelines and compliance monitoring."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
